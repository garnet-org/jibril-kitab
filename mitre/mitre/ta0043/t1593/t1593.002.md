---
description: Search Engines [T1593.002]
icon: magnifying-glass
---

# Search Engines

## Information

* Name: Search Engines
* ID: T1593.002
* Tactics: [TA0043](../)
* Technique: [T1593](./)

## Introduction

Search Engines (T1593.002) is a sub-technique within the MITRE ATT\&CK framework that falls under the broader category of Search Open Websites/Domains (T1593). This sub-technique specifically involves adversaries leveraging publicly available search engines to collect sensitive information, identify vulnerabilities, enumerate exposed infrastructure, and gather intelligence on targeted organizations. Attackers frequently utilize advanced search operators and specialized search queries to efficiently pinpoint sensitive documents, configuration files, exposed services, or other valuable intelligence that can facilitate further exploitation.

## Deep Dive Into Technique

Attackers commonly utilize search engines such as Google, Bing, Shodan, Censys, and others to perform reconnaissance tasks. These platforms index vast amounts of publicly accessible information, allowing attackers to efficiently identify and target vulnerable or misconfigured systems.

Technical execution methods include:

* **Google Dorking**: Attackers use advanced search operators (such as `site:`, `filetype:`, `intitle:`, and `inurl:`) to precisely identify sensitive information or vulnerabilities.
  * Example: `site:example.com filetype:pdf confidential` to find confidential documents.
* **Shodan/Censys searches**: Specialized search engines designed specifically to index internet-connected devices and services. Attackers use these tools to find exposed infrastructure, open ports, vulnerable software versions, and misconfigured devices.
  * Example: Searching Shodan for `port:3389` to identify publicly exposed Remote Desktop Protocol (RDP) endpoints.
* **Automated scraping and scripting**: Attackers may automate the reconnaissance phase by scripting searches, parsing results, and systematically cataloging data for further exploitation.
* **Cached and archived data**: Attackers leverage cached pages or archived content (e.g., via Google cache or the Wayback Machine) to access previously exposed sensitive data or historical information, even after the original content has been removed or secured.

Real-world procedures typically involve:

* Initial reconnaissance phase to build organizational profiles.
* Identification of weak points, exposure of sensitive documents, or configuration files.
* Gathering intelligence to plan subsequent phases, such as initial access, privilege escalation, or lateral movement.

## When this Technique is Usually Used

Attackers commonly employ this sub-technique during the reconnaissance and intelligence-gathering phases of an attack lifecycle. Typical scenarios include:

* **Initial Reconnaissance**:
  * Attackers gather preliminary intelligence about targeted organizations to map their external footprint.
  * Identify publicly exposed sensitive documents, credentials, or configuration files.
* **Vulnerability Discovery**:
  * Attackers identify potential vulnerabilities by searching for configuration files, exposed administrative interfaces, or known vulnerable software versions.
* **Pre-attack Planning and Target Selection**:
  * Attackers use gathered intelligence to select vulnerable targets, prioritize exploitation efforts, and tailor their attack strategies.
* **Ongoing Intelligence Collection**:
  * Continuous monitoring of targeted organizations to detect changes in infrastructure, new vulnerabilities, or updated sensitive documents.
* **Post-compromise Activity**:
  * Attackers may revisit search engines post-compromise to gather additional intelligence to facilitate lateral movement, privilege escalation, or persistence.

## How this Technique is Usually Detected

Detection of search engine reconnaissance activities can be challenging due to their passive nature, but organizations can implement several proactive measures and indicators to detect potential reconnaissance attempts:

* **Monitoring Web Server Logs**:
  * Regularly analyze web server logs for unusual patterns, such as frequent access attempts to sensitive files or pages by search engine crawlers.
* **Honeytokens and Canary Documents**:
  * Deploy fake sensitive documents or credentials (honeytokens) indexed by search engines, and monitor access attempts or usage.
* **Intrusion Detection and Prevention Systems (IDS/IPS)**:
  * Configure IDS/IPS rules to detect suspicious traffic patterns or automated scanning activities originating from suspicious IP addresses or user agents.
* **Search Engine Alerting**:
  * Utilize services such as Google Alerts, Shodan Monitor, or other monitoring tools to receive alerts when sensitive organizational data appears in search engine results.
* **Threat Intelligence Platforms**:
  * Integrate threat intelligence feeds and platforms that track adversary reconnaissance techniques, known malicious IP addresses, or search engine scraping tools.
* **Specific Indicators of Compromise (IoCs)**:
  * Unusual spikes in HTTP requests from search engine crawlers.
  * Access attempts to non-public directories or sensitive files.
  * Automated user-agent strings or IP addresses associated with known reconnaissance bots and scripts.

## Why it is Important to Detect This Technique

Early detection of adversaries utilizing search engines for reconnaissance is critical due to the following potential impacts and risks:

* **Exposure of Sensitive Information**:
  * Attackers may discover confidential documents, credentials, or proprietary data that can directly lead to breaches or data theft.
* **Identification of Vulnerabilities and Misconfigurations**:
  * Attackers gain insights into vulnerable systems, open ports, or misconfigured services, significantly increasing the likelihood of successful exploitation.
* **Enhanced Targeting and Precision of Attacks**:
  * Adversaries who successfully gather detailed intelligence can execute highly targeted and sophisticated attacks, increasing their chances of success and evading detection.
* **Reputation and Compliance Risks**:
  * Public exposure of sensitive data can lead to regulatory compliance violations, substantial financial penalties, and reputational damage.
* **Early Warning and Prevention**:
  * Detecting reconnaissance activities early enables organizations to proactively remediate exposures, harden defenses, and prevent breaches before attackers progress further into the attack lifecycle.

## Examples

Real-world examples demonstrating the use of search engines for reconnaissance include:

* **Operation Aurora (2009-2010)**:
  * Attackers leveraged Google searches (Google Dorking) to identify and exploit vulnerable web applications within targeted organizations.
  * Impact: Significant intellectual property theft, particularly from high-tech companies.
* **Shodan and IoT Device Discovery**:
  * Attackers routinely use Shodan to identify exposed IoT devices, webcams, industrial control systems, and network devices.
  * Impact: Unauthorized access, device hijacking, data exfiltration, and potential disruption of critical infrastructure.
* **FIN7 Cybercrime Group Activities**:
  * FIN7 utilized search engines to enumerate publicly available infrastructure, identify exposed administrative interfaces, and gather intelligence to plan large-scale financial attacks.
  * Impact: Millions of dollars stolen from financial institutions, retail chains, and hospitality organizations.
* **Mirai Botnet (2016)**:
  * Attackers used Shodan and similar search engines to identify vulnerable IoT devices with default credentials or misconfigurations.
  * Impact: Massive DDoS attacks against major internet services, causing widespread outages and disruptions.
* **APT28 (Fancy Bear / Sofacy)**:
  * Known to leverage search engine reconnaissance techniques to identify publicly exposed data and vulnerabilities within targeted organizations.
  * Impact: Espionage, targeted phishing campaigns, credential theft, and political interference activities.
